# MNV-17 Qwen2.5-Omni Demo

[**ä¸­æ–‡ç‰ˆæœ¬**](#ä¸­æ–‡ç‰ˆæœ¬) | [**English Version**](#english-version)

---

## ä¸­æ–‡ç‰ˆæœ¬

### MNV-17 Qwen2.5-Omni æ¼”ç¤ºï¼šéè¯­è¨€å‘å£°è¯†åˆ«

æœ¬ä»“åº“å±•ç¤ºäº†åœ¨ [MNV-17 æ•°æ®é›†](https://huggingface.co/datasets/maimai11/MNV_17) ä¸Šå¾®è°ƒçš„ Qwen2.5-Omni æ¨¡å‹åœ¨éè¯­è¨€å‘å£°ï¼ˆNVï¼‰ASRè¯†åˆ«ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚

**[ç‚¹å‡»è¿™é‡Œä½“éªŒå¯æ’­æ”¾çš„éŸ³é¢‘æ¼”ç¤º](https://yongaifadian1.github.io/MNV-17/)**

### å…³é”®å‘ç°

#### æœªè§è¯´è¯äººæ³›åŒ–èƒ½åŠ›

**é‡è¦è¯´æ˜**: æ‰€æœ‰æ¼”ç¤ºæ ·æœ¬çš„è¯´è¯äººåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨æœªè§ã€‚

è¿™è¯æ˜äº†æ¨¡å‹å­¦ä¼šäº†**é€šç”¨çš„NVå‘å£°è§„å¾‹**ï¼Œè€Œéä»…ä»…æ‹Ÿåˆç‰¹å®šè¯´è¯äººçš„å‘å£°ä¹ æƒ¯ï¼Œå±•ç°äº†ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚

### æ¨¡å‹æ€§èƒ½

æ ¹æ®æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2509.18196)å®éªŒç»“æœï¼š

| æ¨¡å‹ Model | è”åˆCER | NVè¯†åˆ«å‡†ç¡®ç‡ |
|------------|---------|-------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

#### æ€§èƒ½äº®ç‚¹

1. **æœ€ä½è”åˆé”™è¯¯ç‡**: Qwen2.5-Omni å®ç°äº† 3.60% çš„è”åˆå­—ç¬¦é”™è¯¯ç‡ï¼Œåœ¨ ASR å’Œ NV è¯†åˆ«åŒé‡ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚
2. **å“è¶ŠNVè¯†åˆ«**: åœ¨ä¸¥æ ¼çš„å®Œå…¨åŒ¹é…è¯„ä¼°ä¸‹è¾¾åˆ° 57.29% å‡†ç¡®ç‡ï¼ˆç±»å‹ã€æ•°é‡ã€é¡ºåºå®Œå…¨åŒ¹é…ï¼‰ã€‚

### æ¼”ç¤ºæ ·æœ¬

ä»¥ä¸‹å±•ç¤ºäº†åœ¨MNV-17ä¸ŠæŒ‡ä»¤å¾®è°ƒçš„Qwen2.5-Omniæ¨¡å‹åœ¨æ¼”ç¤ºæ ·æœ¬ä¸Šçš„é¢„æµ‹ç»“æœï¼š

#### æ ·æœ¬ 1
- **æ¨¡å‹é¢„æµ‹**: è¿™ä¸ªç†è®ºçš„æ‚–è®ºä¹‹å¤„åœ¨äº [cough] è¯·å¤§å®¶åŸè°…ï¼Œåœ¨äºå®ƒçš„å‰ææœ¬èº«â€”â€”å“å‘€è¿™ç²‰ç¬”ç°çœŸæ˜¯ [sneeze]â€”â€”å®ƒçš„å‰ææœ¬èº«å°±åŒ…å«äº†ç»“è®ºï¼Œè¿™å¬èµ·æ¥æ˜¯ä¸æ˜¯å¾ˆ [chuckle] è’è°¬ã€‚

#### æ ·æœ¬ 2
- **æ¨¡å‹é¢„æµ‹**: æˆ‘ç»™è‡ªå·±æ³¡äº†æ¯çƒ­èŒ¶ [hum]ï¼Œåˆä»å†°ç®±é‡Œæ‹¿å‡ºé‚£å—æ˜¨å¤©æ²¡èˆå¾—åƒçš„ææ‹‰ç±³è‹ [smack]ï¼Œç„¶åçªåœ¨æ²™å‘é‡Œåˆ·ç€æç¬‘è§†é¢‘ [laugh]ï¼Œè¿™å¤§æ¦‚å°±æ˜¯æœ€ç®€å•çš„å¹¸ç¦å§ã€‚

#### æ ·æœ¬ 3
- **æ¨¡å‹é¢„æµ‹**: ä»–å¼€ç€é‚£è¾†æ–°ä¹°çš„è·‘è½¦ [whistle] ä»æˆ‘èº«è¾¹å‘¼å•¸è€Œè¿‡ï¼Œæ•…æ„åœ¨æˆ‘é¢å‰åœä¸‹ [cough] åšå‡ºä¸€ä¸ªè‡ªä»¥ä¸ºå¾ˆå¸…çš„å§¿åŠ¿ï¼Œæˆ‘å½“åœº [clap] å°±ç»™ä»–é¼“äº†é¼“æŒï¼Œè¦å¤šæ•·è¡æœ‰å¤šæ•·è¡ã€‚

#### æ ·æœ¬ 4
- **æ¨¡å‹é¢„æµ‹**: æ™šé¥­æ—¶åœºé¢ä¸€åº¦éå¸¸çƒ­é—¹ï¼Œæˆ‘å®¶çš„çŒ«[hiss]ä¸è®©æ–°æ¥çš„å®¢äººé è¿‘ï¼Œå°ä¾„å¥³è¡¨æ¼”å®ŒèŠ‚ç›®åå¤§å®¶[applaud]çƒ­æƒ…é¼“æŒï¼Œè€Œå¯¹èƒ¡æ¤’ç²‰è¿‡æ•çš„çˆ·çˆ·åˆ™[sneeze]æ‰“èµ·äº†å–·åšã€‚

#### æ ·æœ¬ 5
- **æ¨¡å‹é¢„æµ‹**: æˆ‘ä¸€èµ°è¿›é‚£ä¸ªç§¯æ»¡ç°å°˜çš„é˜æ¥¼ [sneeze] å°±å¿ä¸ä½æ‰“äº†ä¸ªå¤§å–·åšï¼Œé¢å¯¹ç€å †ç§¯å¦‚å±±çš„æ‚ç‰© [sigh] çœŸä¸çŸ¥ä»ä½•ä¸‹æ‰‹ï¼Œä½†å½“æˆ‘ä»æ—§ç®±å­é‡Œç¿»å‡ºé‚£å¼ ç»ç‰ˆé»‘èƒ¶å”±ç‰‡æ—¶ï¼Œæˆ‘ç®€ç›´æƒ³ä¸ºè‡ªå·±çš„å¥½è¿ [clap] æ¬¢å‘¼ï¼

#### æ ·æœ¬ 6
- **æ¨¡å‹é¢„æµ‹**: è€æ•™æˆè®²åˆ°ä»–å¹´è½»æ—¶çš„ç³—äº‹ï¼Œå…¨ç­ [laugh] éƒ½ç¬‘å¾—å‰ä»°ååˆï¼Œå¯ä»–ä¸€æåˆ°è¿™å‘¨æœ«çš„ä½œä¸š [moan] æˆ‘çš„å¤´å°±å¼€å§‹ç–¼äº†ï¼Œè€Œä¸”è¿™æ•™å®¤çš„ç°å°˜ä¹Ÿå¤ªå¤šäº† [sneeze]ï¼ŒçœŸæ˜¯è®©äººå—ä¸äº†ã€‚

#### æ ·æœ¬ 7
- **æ¨¡å‹é¢„æµ‹**: çœ‹ç€ç”µå½±é‡Œä¸»è§’ç‰ºç‰²çš„ç”»é¢ï¼Œå½±é™¢é‡Œä¸€ç‰‡ [sniffle] çš„å•œæ³£å£°ï¼Œæˆ‘å´åœ¨å†…å¿ƒä¸ºä»–çš„å£®ä¸¾ [applaud] å–å½©ï¼Œç„¶åé•¿å¹ä¸€å£æ°” [hum]ï¼Œæ€è€ƒç€äººæ€§çš„å¤æ‚ã€‚

#### æ ·æœ¬ 8
- **æ¨¡å‹é¢„æµ‹**: å›æƒ³èµ·é‚£äº›ç‹¬è‡ªå¥‹æ–—çš„å¤œæ™š [sigh]ï¼ŒçœŸä¸çŸ¥é“æ˜¯æ€ä¹ˆç†¬è¿‡æ¥çš„ï¼Œä½†å½“æˆ‘æ˜¨å¤©ç»ˆäºæ”»å…‹é‚£ä¸ªéš¾é¢˜æ—¶ [clap]ï¼Œæˆ‘çŸ¥é“æ‰€æœ‰çš„ä»˜å‡ºéƒ½å€¼å¾—äº†ï¼Œæœ€ç»ˆé¡¹ç›®æˆåŠŸå‘å¸ƒé‚£ä¸€åˆ» [applaud]ï¼Œæˆ‘æ„Ÿå—åˆ°äº†å‰æ‰€æœªæœ‰çš„æˆå°±æ„Ÿã€‚

#### æ ·æœ¬ 9
- **æ¨¡å‹é¢„æµ‹**: ä»–æœ€ç»ˆ [exhale] æ”¾ä¸‹äº†æ‰‹ä¸­çš„æ—§ç…§ç‰‡ï¼Œå¿ƒæƒ³æˆ‘ä»¬ [sigh] ç»ˆç©¶è¿˜æ˜¯èµ°åˆ°äº†è¿™ä¸€æ­¥ï¼Œä¸€å¿µåŠæ­¤ï¼Œåˆæƒ³èµ·å¥¹æœ€åé‚£äº›è¯ [hiss]ï¼ŒçœŸæ˜¯åˆ»è–„åˆä¼¤äººã€‚

### æ•°æ®é›†ç‰¹ç‚¹

#### MNV-17 æ•°æ®é›†ä¼˜åŠ¿

1. **è¡¨æ¼”å¼å½•åˆ¶**: é¿å…äº†è‡ªç„¶è¯­éŸ³ä¸­NVçš„æ¨¡ç³Šæ€§ï¼Œç¡®ä¿é«˜è´¨é‡æ ‡æ³¨ã€‚
2. **ç±»åˆ«å¹³è¡¡**: 17ä¸ªNVç±»åˆ«åˆ†å¸ƒå‡è¡¡ï¼ˆæœ€å¤§æœ€å°æ¯”ä¾‹ä»…2.7ï¼‰ã€‚
3. **è¯´è¯äººå¤šæ ·æ€§**: 49åæ¥è‡ªä¸åŒåœ°åŒºçš„ä¸­æ–‡æ¯è¯­è€…ã€‚
4. **è¯­å¢ƒä¸°å¯Œ**: NVè‡ªç„¶åµŒå…¥åœ¨è¯­ä¹‰ä¸°å¯Œçš„å¥å­ä¸­ã€‚

#### è®¾è®¡åˆ›æ–°

- **è„šæœ¬åŒ–æ–¹æ³•**: ä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­å¢ƒï¼Œç¡®ä¿NVçš„è¯­ä¹‰åˆç†æ€§ã€‚
- **å¤šNVç»„åˆ**: æ”¯æŒ1-3ä¸ªNVçš„éšæœºç»„åˆï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚
- **è¯´è¯äººç‹¬ç«‹åˆ’åˆ†**: ä¸¥æ ¼çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†ï¼Œç¡®ä¿æ³›åŒ–è¯„ä¼°ã€‚

### ç›¸å…³é“¾æ¥

- ğŸ“„ **è®ºæ–‡**: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- ğŸ¤— **æ•°æ®é›†**: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- ğŸ¤– **å¾®è°ƒæ¨¡å‹**: [å³å°†å‘å¸ƒ]

### å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬æ•°æ®é›†æˆ–ç›¸å…³å·¥ä½œï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech},
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

### ä¸»è¦è´¡çŒ®

1. **é«˜è´¨é‡æ•°æ®é›†**: MNV-17æä¾›äº†ç›®å‰ç±»åˆ«æœ€å…¨é¢çš„ä¸­æ–‡éè¯­è¨€å‘å£°æ•°æ®é›†ã€‚
2. **æ³›åŒ–èƒ½åŠ›éªŒè¯**: è¯æ˜äº†åŸºäºè¡¨æ¼”å¼æ•°æ®çš„æ¨¡å‹å…·æœ‰ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚
3. **ååŒæ•ˆåº”**: è¯æ˜NVè¯†åˆ«èƒ½åŠ›åœ¨å¤šä»»åŠ¡è®­ç»ƒä¸‹çš„è¯­éŸ³æ¨¡å‹ä¸­ä¸ä¼šæŸå®³ASRæ€§èƒ½ã€‚
4. **åŸºå‡†å»ºç«‹**: ä¸ºNVæ„ŸçŸ¥ASRå»ºç«‹äº†æ ‡å‡†è¯„ä¼°åŸºå‡†ã€‚

---

**ä½œè€…**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**æœºæ„**: åå—ç†å·¥å¤§å­¦, é¦™æ¸¯ç†å·¥å¤§å­¦, åŒæµå¤§å­¦, ä¸Šæµ·äº¤é€šå¤§å­¦, é¦™æ¸¯ä¸­æ–‡å¤§å­¦, ä½›å±±å¤§å­¦

---

## English Version

### MNV-17 Qwen2.5-Omni Demo: Nonverbal Vocalization Recognition

This repository demonstrates the excellent performance of Qwen2.5-Omni model fine-tuned on the [MNV-17 dataset](https://huggingface.co/datasets/maimai11/MNV_17) for Nonverbal Vocalization (NV) ASR recognition tasks.

**[Click here for interactive audio demo](https://yongaifadian1.github.io/MNV-17/)**

### Key Findings

#### Unseen Speaker Generalization

**Crucial Note**: All demo samples are from speakers who were completely unseen during training.

This demonstrates that the model learned **universal NV vocalization patterns** rather than merely fitting specific speakers' habits, showcasing excellent cross-speaker generalization.

### Model Performance

According to our [paper](https://arxiv.org/abs/2509.18196) experimental results:

| Model | Joint CER | NV Recognition Accuracy |
|-------|-----------|------------------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

#### Performance Highlights

1. **Lowest Joint Error Rate**: Qwen2.5-Omni achieved 3.60% joint CER, best performance in dual ASR and NV recognition tasks.
2. **Excellent NV Recognition**: 57.29% accuracy under strict exact-match evaluation (type, count, order must all match).

### Demo Samples

The following shows the prediction results of Qwen2.5-Omni model fine-tuned on MNV-17 dataset on demo samples:

#### Sample 1
- **Model Prediction**: è¿™ä¸ªç†è®ºçš„æ‚–è®ºä¹‹å¤„åœ¨äº [cough] è¯·å¤§å®¶åŸè°…ï¼Œåœ¨äºå®ƒçš„å‰ææœ¬èº«â€”â€”å“å‘€è¿™ç²‰ç¬”ç°çœŸæ˜¯ [sneeze]â€”â€”å®ƒçš„å‰ææœ¬èº«å°±åŒ…å«äº†ç»“è®ºï¼Œè¿™å¬èµ·æ¥æ˜¯ä¸æ˜¯å¾ˆ [chuckle] è’è°¬ã€‚

#### Sample 2
- **Model Prediction**: æˆ‘ç»™è‡ªå·±æ³¡äº†æ¯çƒ­èŒ¶ [hum]ï¼Œåˆä»å†°ç®±é‡Œæ‹¿å‡ºé‚£å—æ˜¨å¤©æ²¡èˆå¾—åƒçš„ææ‹‰ç±³è‹ [smack]ï¼Œç„¶åçªåœ¨æ²™å‘é‡Œåˆ·ç€æç¬‘è§†é¢‘ [laugh]ï¼Œè¿™å¤§æ¦‚å°±æ˜¯æœ€ç®€å•çš„å¹¸ç¦å§ã€‚

#### Sample 3
- **Model Prediction**: ä»–å¼€ç€é‚£è¾†æ–°ä¹°çš„è·‘è½¦ [whistle] ä»æˆ‘èº«è¾¹å‘¼å•¸è€Œè¿‡ï¼Œæ•…æ„åœ¨æˆ‘é¢å‰åœä¸‹ [cough] åšå‡ºä¸€ä¸ªè‡ªä»¥ä¸ºå¾ˆå¸…çš„å§¿åŠ¿ï¼Œæˆ‘å½“åœº [clap] å°±ç»™ä»–é¼“äº†é¼“æŒï¼Œè¦å¤šæ•·è¡æœ‰å¤šæ•·è¡ã€‚

#### Sample 4
- **Model Prediction**: æ™šé¥­æ—¶åœºé¢ä¸€åº¦éå¸¸çƒ­é—¹ï¼Œæˆ‘å®¶çš„çŒ«[hiss]ä¸è®©æ–°æ¥çš„å®¢äººé è¿‘ï¼Œå°ä¾„å¥³è¡¨æ¼”å®ŒèŠ‚ç›®åå¤§å®¶[applaud]çƒ­æƒ…é¼“æŒï¼Œè€Œå¯¹èƒ¡æ¤’ç²‰è¿‡æ•çš„çˆ·çˆ·åˆ™[sneeze]æ‰“èµ·äº†å–·åšã€‚

#### Sample 5
- **Model Prediction**: æˆ‘ä¸€èµ°è¿›é‚£ä¸ªç§¯æ»¡ç°å°˜çš„é˜æ¥¼ [sneeze] å°±å¿ä¸ä½æ‰“äº†ä¸ªå¤§å–·åšï¼Œé¢å¯¹ç€å †ç§¯å¦‚å±±çš„æ‚ç‰© [sigh] çœŸä¸çŸ¥ä»ä½•ä¸‹æ‰‹ï¼Œä½†å½“æˆ‘ä»æ—§ç®±å­é‡Œç¿»å‡ºé‚£å¼ ç»ç‰ˆé»‘èƒ¶å”±ç‰‡æ—¶ï¼Œæˆ‘ç®€ç›´æƒ³ä¸ºè‡ªå·±çš„å¥½è¿ [clap] æ¬¢å‘¼ï¼

#### Sample 6
- **Model Prediction**: è€æ•™æˆè®²åˆ°ä»–å¹´è½»æ—¶çš„ç³—äº‹ï¼Œå…¨ç­ [laugh] éƒ½ç¬‘å¾—å‰ä»°ååˆï¼Œå¯ä»–ä¸€æåˆ°è¿™å‘¨æœ«çš„ä½œä¸š [moan] æˆ‘çš„å¤´å°±å¼€å§‹ç–¼äº†ï¼Œè€Œä¸”è¿™æ•™å®¤çš„ç°å°˜ä¹Ÿå¤ªå¤šäº† [sneeze]ï¼ŒçœŸæ˜¯è®©äººå—ä¸äº†ã€‚

#### Sample 7
- **Model Prediction**: çœ‹ç€ç”µå½±é‡Œä¸»è§’ç‰ºç‰²çš„ç”»é¢ï¼Œå½±é™¢é‡Œä¸€ç‰‡ [sniffle] çš„å•œæ³£å£°ï¼Œæˆ‘å´åœ¨å†…å¿ƒä¸ºä»–çš„å£®ä¸¾ [applaud] å–å½©ï¼Œç„¶åé•¿å¹ä¸€å£æ°” [hum]ï¼Œæ€è€ƒç€äººæ€§çš„å¤æ‚ã€‚

#### Sample 8
- **Model Prediction**: å›æƒ³èµ·é‚£äº›ç‹¬è‡ªå¥‹æ–—çš„å¤œæ™š [sigh]ï¼ŒçœŸä¸çŸ¥é“æ˜¯æ€ä¹ˆç†¬è¿‡æ¥çš„ï¼Œä½†å½“æˆ‘æ˜¨å¤©ç»ˆäºæ”»å…‹é‚£ä¸ªéš¾é¢˜æ—¶ [clap]ï¼Œæˆ‘çŸ¥é“æ‰€æœ‰çš„ä»˜å‡ºéƒ½å€¼å¾—äº†ï¼Œæœ€ç»ˆé¡¹ç›®æˆåŠŸå‘å¸ƒé‚£ä¸€åˆ» [applaud]ï¼Œæˆ‘æ„Ÿå—åˆ°äº†å‰æ‰€æœªæœ‰çš„æˆå°±æ„Ÿã€‚

#### Sample 9
- **Model Prediction**: ä»–æœ€ç»ˆ [exhale] æ”¾ä¸‹äº†æ‰‹ä¸­çš„æ—§ç…§ç‰‡ï¼Œå¿ƒæƒ³æˆ‘ä»¬ [sigh] ç»ˆç©¶è¿˜æ˜¯èµ°åˆ°äº†è¿™ä¸€æ­¥ï¼Œä¸€å¿µåŠæ­¤ï¼Œåˆæƒ³èµ·å¥¹æœ€åé‚£äº›è¯ [hiss]ï¼ŒçœŸæ˜¯åˆ»è–„åˆä¼¤äººã€‚

### Dataset Characteristics

#### MNV-17 Dataset Advantages

1. **Performative Recording**: Avoids ambiguity of NVs in spontaneous speech, ensures high-quality annotation.
2. **Class Balance**: 17 NV categories with balanced distribution (max/min ratio only 2.7).
3. **Speaker Diversity**: 49 native Mandarin speakers from various regions.
4. **Rich Context**: NVs naturally embedded in semantically rich sentences.

#### Design Innovation

- **Scripted Approach**: LLM-generated natural contexts ensure semantic reasonableness of NVs.
- **Multi-NV Combinations**: Supports random combinations of 1-3 NVs, simulating real scenarios.
- **Speaker-Independent Split**: Strict train/validation/test division ensures generalization evaluation.

### Related Links

- ğŸ“„ **Paper**: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- ğŸ¤— **Dataset**: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- ğŸ¤– **Fine-tuned Model**: [Coming Soon]

### Citation

If you use this dataset or related work, please cite:

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech},
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

### Key Contributions

1. **High-Quality Dataset**: MNV-17 provides the most comprehensive Chinese nonverbal vocalization dataset.
2. **Generalization Verification**: Proves excellent cross-speaker generalization of performative data-based models.
3. **Synergy Discovery**: Proves NV recognition can enhance rather than harm ASR performance.
4. **Benchmark Establishment**: Establishes standard evaluation benchmark for NV-aware ASR.

---

**Authors**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**Institutions**: South China University of Technology, The Hong Kong Polytechnic University, Tongji University, Shanghai Jiao Tong University, The Chinese University of Hong Kong, Foshan University