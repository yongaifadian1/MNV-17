# MNV-17 Qwen2.5-Omni Demo

**ä¸­æ–‡å‰¯è¯­è¨€è¯†åˆ«æ¼”ç¤º**

æœ¬ä»“åº“å±•ç¤ºäº†åœ¨ [MNV-17 æ•°æ®é›†](https://huggingface.co/datasets/maimai11/MNV_17) ä¸Šå¾®è°ƒçš„ Qwen2.5-Omni æ¨¡å‹åœ¨éè¯­è¨€å‘å£°ï¼ˆNVï¼‰ASRè¯†åˆ«ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚

**Nonverbal Vocalization Recognition Demo**

This repository demonstrates the excellent performance of Qwen2.5-Omni model fine-tuned on the [MNV-17 dataset](https://huggingface.co/datasets/maimai11/MNV_17) for Nonverbal Vocalization (NV) ASR recognition tasks.

## ğŸµ åœ¨çº¿æ¼”ç¤º Online Demo

**[ç‚¹å‡»è¿™é‡Œä½“éªŒå¯æ’­æ”¾çš„éŸ³é¢‘æ¼”ç¤º | Click here for interactive audio demo](https://yongaifadian1.github.io/MNV-17-Demo/)**

## å…³é”®å‘ç° Key Findings

### æœªè§è¯´è¯äººæ³›åŒ–èƒ½åŠ› Unseen Speaker Generalization

**é‡è¦è¯´æ˜**: æ‰€æœ‰æ¼”ç¤ºæ ·æœ¬çš„è¯´è¯äººåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨æœªè§

**Crucial Note**: All demo samples are from speakers who were completely unseen during training

è¿™è¯æ˜äº†æ¨¡å‹å­¦ä¼šäº†**é€šç”¨çš„NVå‘å£°è§„å¾‹**ï¼Œè€Œéä»…ä»…æ‹Ÿåˆç‰¹å®šè¯´è¯äººçš„å‘å£°ä¹ æƒ¯ï¼Œå±•ç°äº†ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚

This demonstrates that the model learned **universal NV vocalization patterns** rather than merely fitting specific speakers' habits, showcasing excellent cross-speaker generalization.

## æ¨¡å‹æ€§èƒ½ Model Performance

æ ¹æ®æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2509.18196)å®éªŒç»“æœï¼š

According to our [paper](https://arxiv.org/abs/2509.18196) experimental results:

| æ¨¡å‹ Model | è”åˆCER Joint CER | NVè¯†åˆ«å‡†ç¡®ç‡ NV Accuracy | 
|------------|-------------------|--------------------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

### æ€§èƒ½äº®ç‚¹ Performance Highlights

1. **æœ€ä½è”åˆé”™è¯¯ç‡**: Qwen2.5-Omni å®ç°äº† 3.60% çš„è”åˆå­—ç¬¦é”™è¯¯ç‡ï¼Œåœ¨ ASR å’Œ NV è¯†åˆ«åŒé‡ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³
2. **å“è¶ŠNVè¯†åˆ«**: åœ¨ä¸¥æ ¼çš„å®Œå…¨åŒ¹é…è¯„ä¼°ä¸‹è¾¾åˆ° 57.29% å‡†ç¡®ç‡ï¼ˆç±»å‹ã€æ•°é‡ã€é¡ºåºå®Œå…¨åŒ¹é…ï¼‰

1. **Lowest Joint Error Rate**: Qwen2.5-Omni achieved 3.60% joint CER, best performance in dual ASR and NV recognition tasks
2. **Excellent NV Recognition**: 57.29% accuracy under strict exact-match evaluation (type, count, order must all match)

## æ•°æ®é›†ç‰¹ç‚¹ Dataset Characteristics

### MNV-17 æ•°æ®é›†ä¼˜åŠ¿ MNV-17 Dataset Advantages

1. **è¡¨æ¼”å¼å½•åˆ¶**: é¿å…äº†è‡ªç„¶è¯­éŸ³ä¸­NVçš„æ¨¡ç³Šæ€§ï¼Œç¡®ä¿é«˜è´¨é‡æ ‡æ³¨
2. **ç±»åˆ«å¹³è¡¡**: 17ä¸ªNVç±»åˆ«åˆ†å¸ƒå‡è¡¡ï¼ˆæœ€å¤§æœ€å°æ¯”ä¾‹ä»…2.7ï¼‰
3. **è¯´è¯äººå¤šæ ·æ€§**: 49åæ¥è‡ªä¸åŒåœ°åŒºçš„ä¸­æ–‡æ¯è¯­è€…
4. **è¯­å¢ƒä¸°å¯Œ**: NVè‡ªç„¶åµŒå…¥åœ¨è¯­ä¹‰ä¸°å¯Œçš„å¥å­ä¸­

1. **Performative Recording**: Avoids ambiguity of NVs in spontaneous speech, ensures high-quality annotation
2. **Class Balance**: 17 NV categories with balanced distribution (max/min ratio only 2.7)
3. **Speaker Diversity**: 49 native Mandarin speakers from various regions  
4. **Rich Context**: NVs naturally embedded in semantically rich sentences

è‰¯å¥½çš„æ•°æ®é›†è®¾è®¡å’Œå®è·µæ˜¯ä½¿å…¶èƒ½å¤Ÿåœ¨æ—¶é•¿æœ‰é™çš„æ ·æœ¬ä¸­è®­ç»ƒå‡ºæ³›åŒ–æ€§å¼ºçš„NV ASRæ¨¡å‹çš„å…³é”®åŸå› ä¹‹ä¸€ã€‚

The well-designed dataset and practices are one of the key reasons why it can train generalized NV ASR models with strong generalization from limited-duration samples.

## ç›¸å…³é“¾æ¥ Related Links

- è®ºæ–‡ Paper: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- æ•°æ®é›† Dataset: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- å¾®è°ƒæ¨¡å‹ Fine-tuned Model: [å³å°†å‘å¸ƒ | Coming Soon]

## å¼•ç”¨ Citation

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech}, 
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

---

**ä½œè€… Authors**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**æœºæ„ Institutions**: åå—ç†å·¥å¤§å­¦, é¦™æ¸¯ç†å·¥å¤§å­¦, åŒæµå¤§å­¦, ä¸Šæµ·äº¤é€šå¤§å­¦, é¦™æ¸¯ä¸­æ–‡å¤§å­¦, ä½›å±±å¤§å­¦