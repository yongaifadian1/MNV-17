# MNV-17

[**ä¸­æ–‡ç‰ˆæœ¬**](#ä¸­æ–‡ç‰ˆæœ¬) | [**English Version**](#english-version)

---

## ä¸­æ–‡ç‰ˆæœ¬

### MNV-17ï¼šéžè¯­è¨€å‘å£°è¯†åˆ«

æœ¬ä»“åº“å±•ç¤ºäº†åœ¨ [MNV-17 æ•°æ®é›†](https://huggingface.co/datasets/maimai11/MNV_17) ä¸Šå¾®è°ƒçš„ Qwen2.5-Omni æ¨¡åž‹åœ¨éžè¯­è¨€å‘å£°ï¼ˆNVï¼‰ASRè¯†åˆ«ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨çŽ°ã€‚åŒæ—¶æä¾›äº†Qwen2.5-Omniå’ŒQwen2-Audioçš„æŽ¨ç†è„šæœ¬ã€‚

**[ç‚¹å‡»è¿™é‡Œä½“éªŒå¯æ’­æ”¾çš„éŸ³é¢‘æ¼”ç¤º](https://yongaifadian1.github.io/MNV-17/)**

### å…³é”®å‘çŽ°

#### æœªè§è¯´è¯äººæ³›åŒ–èƒ½åŠ›

**é‡è¦è¯´æ˜Ž**: æ‰€æœ‰æ¼”ç¤ºæ ·æœ¬çš„è¯´è¯äººåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨æœªè§ã€‚

è¿™è¯æ˜Žäº†æ¨¡åž‹å­¦ä¼šäº†**é€šç”¨çš„NVå‘å£°è§„å¾‹**ï¼Œè€Œéžä»…ä»…æ‹Ÿåˆç‰¹å®šè¯´è¯äººçš„å‘å£°ä¹ æƒ¯ï¼Œå±•çŽ°äº†ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚

#### åŸŸå¤–ä¸Žè·¨è¯­ç§æ³›åŒ–

æˆ‘ä»¬æ–°å¢žäº† `audio_samples/out_of_domain/` ä¸Ž `audio_samples/cross_language/` ä¸¤ç»„ç¤ºä¾‹ï¼Œå±•ç¤ºåœ¨ MNV-17 ä¸Šå¾®è°ƒçš„ Qwen2.5-Omni è¿˜èƒ½ï¼š

1. **åŸŸå¤–æ³›åŒ–**ï¼šåœ¨çŽ¯å¢ƒå™ªå£°ã€å™äº‹é£Žæ ¼ç­‰å‡ä¸Žè®­ç»ƒè¯­æ–™ä¸åŒçš„æ ·æœ¬ä¸Šä¿æŒç¨³å®šè¾“å‡ºã€‚
2. **è·¨è¯­ç§æ³›åŒ–**ï¼šåœ¨ä¸­è‹±å¤¹æ‚ä¹ƒè‡³çº¯è‹±æ–‡çš„è¯­éŸ³é‡Œæ­£ç¡®æ’å…¥éžè¯­è¨€æ ‡ç­¾ï¼Œè¯´æ˜Žæ¨¡åž‹å¹¶ä¸ä¾èµ–ç‰¹å®šè¯­ç§è¯­å¢ƒã€‚

> ç¤ºä¾‹éŸ³é¢‘å¯ç›´æŽ¥åœ¨é¡¶ç«¯ Demo é¡µé¢ä¸­æ’­æ”¾ï¼Œæˆ–ä»Ž `audio_samples/out_of_domain` ä¸Ž `audio_samples/cross_language` æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°å¯¹åº” wavã€‚

### æ¨¡åž‹æ€§èƒ½

æ ¹æ®æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2509.18196)å®žéªŒç»“æžœï¼š

| æ¨¡åž‹ Model | è”åˆCER | NVè¯†åˆ«å‡†ç¡®çŽ‡ |
|------------|---------|-------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

#### æ€§èƒ½äº®ç‚¹

1. **æœ€ä½Žè”åˆé”™è¯¯çŽ‡**: Qwen2.5-Omni å®žçŽ°äº† 3.60% çš„è”åˆå­—ç¬¦é”™è¯¯çŽ‡ï¼Œåœ¨ ASR å’Œ NV è¯†åˆ«åŒé‡ä»»åŠ¡ä¸­è¡¨çŽ°æœ€ä½³ã€‚
2. **å“è¶ŠNVè¯†åˆ«**: åœ¨ä¸¥æ ¼çš„å®Œå…¨åŒ¹é…è¯„ä¼°ä¸‹è¾¾åˆ° 57.29% å‡†ç¡®çŽ‡ï¼ˆç±»åž‹ã€æ•°é‡ã€é¡ºåºå®Œå…¨åŒ¹é…ï¼‰ã€‚


### æ•°æ®é›†ç‰¹ç‚¹

#### MNV-17 æ•°æ®é›†ä¼˜åŠ¿

1. **è¡¨æ¼”å¼å½•åˆ¶**: é¿å…äº†è‡ªç„¶è¯­éŸ³ä¸­NVçš„æ¨¡ç³Šæ€§ï¼Œç¡®ä¿é«˜è´¨é‡æ ‡æ³¨ã€‚
2. **ç±»åˆ«å¹³è¡¡**: 17ä¸ªNVç±»åˆ«åˆ†å¸ƒå‡è¡¡ï¼ˆæœ€å¤§æœ€å°æ¯”ä¾‹ä»…2.7ï¼‰ã€‚
3. **è¯´è¯äººå¤šæ ·æ€§**: 49åæ¥è‡ªä¸åŒåœ°åŒºçš„ä¸­æ–‡æ¯è¯­è€…ã€‚
4. **è¯­å¢ƒä¸°å¯Œ**: NVè‡ªç„¶åµŒå…¥åœ¨è¯­ä¹‰ä¸°å¯Œçš„å¥å­ä¸­ã€‚

#### è®¾è®¡åˆ›æ–°

- **è„šæœ¬åŒ–æ–¹æ³•**: ä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­å¢ƒï¼Œç¡®ä¿NVçš„è¯­ä¹‰åˆç†æ€§ã€‚
- **å¤šNVç»„åˆ**: æ”¯æŒ1-3ä¸ªNVçš„éšæœºç»„åˆï¼Œæ¨¡æ‹ŸçœŸå®žåœºæ™¯ã€‚
- **è¯´è¯äººç‹¬ç«‹åˆ’åˆ†**: ä¸¥æ ¼çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†ï¼Œç¡®ä¿æ³›åŒ–è¯„ä¼°ã€‚

### å¿«é€Ÿå¼€å§‹

#### çŽ¯å¢ƒé…ç½®

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### æŽ¨ç†æµ‹è¯•

æœ¬ä»“åº“æä¾›äº†ä¸¤ä¸ªæ¨¡åž‹çš„æŽ¨ç†è„šæœ¬ï¼Œä»¥Qwen2.5-Omniä¸ºä¾‹ï¼š



```bash
cd examples/qwen2.5omni
bash run_qwen25omni.sh
```

é»˜è®¤å‚æ•°ï¼š
- åŸºç¡€æ¨¡åž‹ï¼š`Qwen/Qwen2.5-Omni-7B`ï¼ˆæ”¯æŒè‡ªåŠ¨ä»Ž ModelScope/HuggingFace ä¸‹è½½ï¼‰
- LoRA æƒé‡ï¼š`../../ckpt/qwen25-omni-7b-finetuned/v0-20250914-150113/checkpoint-192`ï¼ˆéœ€ä»Žé“¾æŽ¥ä¸‹è½½ï¼‰
- æµ‹è¯•æ•°æ®ï¼š`../testset/test.jsonl`



#### è‡ªå®šä¹‰å‚æ•°

æ‚¨å¯ä»¥ç›´æŽ¥ç¼–è¾‘ shell è„šæœ¬æˆ–è¿è¡Œæ—¶ä¼ å…¥å‚æ•°ï¼š

```bash
python inference.py \
    --base_model "your/model/path" \
    --checkpoint "your/lora/path" \
    --test_file "your/test/file.jsonl" \
    --output_file "output.jsonl" \
    --num_samples 50 \
    --gpu_id "0"
```

### ç›¸å…³é“¾æŽ¥

- ðŸ“„ **è®ºæ–‡**: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- ðŸ¤— **æ•°æ®é›†**: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- ðŸ¤– **å¾®è°ƒæ¨¡åž‹**: [Qwen2.5-Omni-7B-MNV17](https://huggingface.co/kiiic/MNV-17-Qwen-fintune)

### å¼•ç”¨

å¦‚æžœæ‚¨ä½¿ç”¨äº†æœ¬æ•°æ®é›†æˆ–ç›¸å…³å·¥ä½œï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech},
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

### ä¸»è¦è´¡çŒ®

1. **é«˜è´¨é‡æ•°æ®é›†**: MNV-17æä¾›äº†ç›®å‰ç±»åˆ«æœ€å…¨é¢çš„ä¸­æ–‡éžè¯­è¨€å‘å£°æ•°æ®é›†ã€‚
2. **æ³›åŒ–èƒ½åŠ›éªŒè¯**: è¯æ˜Žäº†åŸºäºŽè¡¨æ¼”å¼æ•°æ®çš„æ¨¡åž‹å…·æœ‰ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚
3. **ååŒæ•ˆåº”**: è¯æ˜ŽNVè¯†åˆ«èƒ½åŠ›åœ¨å¤šä»»åŠ¡è®­ç»ƒä¸‹çš„è¯­éŸ³æ¨¡åž‹ä¸­ä¸ä¼šæŸå®³ASRæ€§èƒ½ã€‚
4. **åŸºå‡†å»ºç«‹**: ä¸ºNVæ„ŸçŸ¥ASRå»ºç«‹äº†æ ‡å‡†è¯„ä¼°åŸºå‡†ã€‚

---

**ä½œè€…**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**æœºæž„**: åŽå—ç†å·¥å¤§å­¦, é¦™æ¸¯ç†å·¥å¤§å­¦, åŒæµŽå¤§å­¦, ä¸Šæµ·äº¤é€šå¤§å­¦, é¦™æ¸¯ä¸­æ–‡å¤§å­¦, ä½›å±±å¤§å­¦

---

## English Version

### MNV-17: Nonverbal Vocalization Recognition

This repository demonstrates the excellent performance of Qwen2.5-Omni and Qwen2-Audio models fine-tuned on the [MNV-17 dataset](https://huggingface.co/datasets/maimai11/MNV_17) for Nonverbal Vocalization (NV) ASR recognition tasks. It also provides inference scripts for Qwen2.5-Omni and Qwen2-Audio.

**[Click here for interactive audio demo](https://yongaifadian1.github.io/MNV-17/)**

### Key Findings

#### Unseen Speaker Generalization

**Crucial Note**: All demo samples are from speakers who were completely unseen during training.

This demonstrates that the model learned **universal NV vocalization patterns** rather than merely fitting specific speakers' habits, showcasing excellent cross-speaker generalization.

#### Out-of-Domain & Cross-Language Generalization

Two new folders, `audio_samples/out_of_domain/` and `audio_samples/cross_language/`, provide evidence that the Qwen2.5-Omni fine-tuned on MNV-17:

1. **Handles domain shift**: Remains robust when background noise, storytelling style, and acoustic conditions differ from the training corpus.
2. **Transfers across languages**: Inserts the right NV tags even in mixed Mandarin-English or fully English utterances.

> All clips are playable on the Demo page (link above) and reside locally under the two folders for offline inspection.

### Model Performance

According to our [paper](https://arxiv.org/abs/2509.18196) experimental results:

| Model | Joint CER | NV Recognition Accuracy |
|-------|-----------|------------------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

#### Performance Highlights

1. **Lowest Joint Error Rate**: Qwen2.5-Omni achieved 3.60% joint CER, best performance in dual ASR and NV recognition tasks.
2. **Excellent NV Recognition**: 57.29% accuracy under strict exact-match evaluation (type, count, order must all match).


### Dataset Characteristics

#### MNV-17 Dataset Advantages

1. **Performative Recording**: Avoids ambiguity of NVs in spontaneous speech, ensures high-quality annotation.
2. **Class Balance**: 17 NV categories with balanced distribution (max/min ratio only 2.7).
3. **Speaker Diversity**: 49 native Mandarin speakers from various regions.
4. **Rich Context**: NVs naturally embedded in semantically rich sentences.

#### Design Innovation

- **Scripted Approach**: LLM-generated natural contexts ensure semantic reasonableness of NVs.
- **Multi-NV Combinations**: Supports random combinations of 1-3 NVs, simulating real scenarios.
- **Speaker-Independent Split**: Strict train/validation/test division ensures generalization evaluation.

### Quick Start

#### Environment Setup

```bash
# Install dependencies
pip install -r requirements.txt
```

#### Inference Testing

This repository provides inference scripts for two models, Qwen2.5-Omni as an example:



```bash
cd examples/qwen2.5omni
bash run_qwen25omni.sh
```

Default parameters:
- Base model: `Qwen/Qwen2.5-Omni-7B` (supports auto-download from ModelScope/HuggingFace)
- LoRA weights: `../../ckpt/qwen25-omni-7b-finetuned/v0-20250914-150113/checkpoint-192` (need to download from link)
- Test data: `../testset/test.jsonl` 




#### Custom Parameters

You can edit the shell scripts directly or pass parameters at runtime:

```bash
python inference.py \
    --base_model "your/model/path" \
    --checkpoint "your/lora/path" \
    --test_file "your/test/file.jsonl" \
    --output_file "output.jsonl" \
    --num_samples 50 \
    --gpu_id "0"

```

### Related Links

- ðŸ“„ **Paper**: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- ðŸ¤— **Dataset**: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- ðŸ¤– **Fine-tuned Model**: [MNV17](https://huggingface.co/kiiic/MNV-17-Qwen-fintune)

### Citation

If you use this dataset or related work, please cite:

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech},
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

### Key Contributions

1. **High-Quality Dataset**: MNV-17 provides the most comprehensive Chinese nonverbal vocalization dataset.
2. **Generalization Verification**: Proves excellent cross-speaker generalization of performative data-based models.
3. **Synergy Discovery**: Proves NV recognition can enhance rather than harm ASR performance.
4. **Benchmark Establishment**: Establishes standard evaluation benchmark for NV-aware ASR.

---

**Authors**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**Institutions**: South China University of Technology, The Hong Kong Polytechnic University, Tongji University, Shanghai Jiao Tong University, The Chinese University of Hong Kong, Foshan University
