# MNV-17 Qwen2.5-Omni Demo: Nonverbal Vocalization Recognition

**ä¸­æ–‡å‰¯è¯­è¨€è¯†åˆ«æ¼”ç¤º | Nonverbal Vocalization Recognition Demo**

æœ¬ä»“åº“å±•ç¤ºäº†åœ¨ [MNV-17 æ•°æ®é›†](https://huggingface.co/datasets/maimai11/MNV_17) ä¸Šå¾®è°ƒçš„ Qwen2.5-Omni æ¨¡å‹åœ¨éè¯­è¨€å‘å£°ï¼ˆNVï¼‰ASRè¯†åˆ«ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚

This repository demonstrates the excellent performance of Qwen2.5-Omni model fine-tuned on the [MNV-17 dataset](https://huggingface.co/datasets/maimai11/MNV_17) for Nonverbal Vocalization (NV) ASR recognition tasks.

## ğŸµ åœ¨çº¿æ¼”ç¤º | Online Demo

**[ç‚¹å‡»è¿™é‡Œä½“éªŒå¯æ’­æ”¾çš„éŸ³é¢‘æ¼”ç¤º | Click here for interactive audio demo](https://yongaifadian1.github.io/MNV-17-Demo/)**

## å…³é”®å‘ç° | Key Findings

### æœªè§è¯´è¯äººæ³›åŒ–èƒ½åŠ› | Unseen Speaker Generalization
- **é‡è¦è¯´æ˜**: æ‰€æœ‰æ¼”ç¤ºéŸ³é¢‘å‡æ¥è‡ªæµ‹è¯•é›†è¯´è¯äººï¼ˆspeaker_F_02, speaker_F_03, speaker_M_02, speaker_M_03ï¼‰ï¼Œè¿™äº›è¯´è¯äººåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨æœªè§
- **Crucial Note**: All demo audio samples are from test set speakers (speaker_F_02, speaker_F_03, speaker_M_02, speaker_M_03), who were completely unseen during training

è¿™è¯æ˜äº†æ¨¡å‹å­¦ä¼šäº†**é€šç”¨çš„NVå‘å£°è§„å¾‹**ï¼Œè€Œéä»…ä»…æ‹Ÿåˆç‰¹å®šè¯´è¯äººçš„å‘å£°ä¹ æƒ¯ï¼Œå±•ç°äº†ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚

This demonstrates that the model learned **universal NV vocalization patterns** rather than merely fitting specific speakers' habits, showcasing excellent cross-speaker generalization.

## æ¨¡å‹æ€§èƒ½ | Model Performance

æ ¹æ®æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2509.18196)å®éªŒç»“æœï¼š

According to our [paper](https://arxiv.org/abs/2509.18196) experimental results:

| æ¨¡å‹ Model | è”åˆCER | NVè¯†åˆ«å‡†ç¡®ç‡ | 
|------------|---------|-------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

### æ€§èƒ½äº®ç‚¹ | Performance Highlights

1. **æœ€ä½è”åˆé”™è¯¯ç‡**: Qwen2.5-Omni å®ç°äº† 3.60% çš„è”åˆå­—ç¬¦é”™è¯¯ç‡ï¼Œåœ¨ ASR å’Œ NV è¯†åˆ«åŒé‡ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³
2. **å“è¶ŠNVè¯†åˆ«**: åœ¨ä¸¥æ ¼çš„å®Œå…¨åŒ¹é…è¯„ä¼°ä¸‹è¾¾åˆ° 57.29% å‡†ç¡®ç‡ï¼ˆç±»å‹ã€æ•°é‡ã€é¡ºåºå®Œå…¨åŒ¹é…ï¼‰

1. **Lowest Joint Error Rate**: Qwen2.5-Omni achieved 3.60% joint CER, best performance in dual ASR and NV recognition tasks
2. **Excellent NV Recognition**: 57.29% accuracy under strict exact-match evaluation (type, count, order must all match)

## æ•°æ®é›†ç‰¹ç‚¹ | Dataset Characteristics

### MNV-17 æ•°æ®é›†ä¼˜åŠ¿ | MNV-17 Dataset Advantages

1. **è¡¨æ¼”å¼å½•åˆ¶**: é¿å…äº†è‡ªç„¶è¯­éŸ³ä¸­NVçš„æ¨¡ç³Šæ€§ï¼Œç¡®ä¿é«˜è´¨é‡æ ‡æ³¨
2. **ç±»åˆ«å¹³è¡¡**: 17ä¸ªNVç±»åˆ«åˆ†å¸ƒå‡è¡¡ï¼ˆæœ€å¤§æœ€å°æ¯”ä¾‹ä»…2.7ï¼‰
3. **è¯´è¯äººå¤šæ ·æ€§**: 49åæ¥è‡ªä¸åŒåœ°åŒºçš„ä¸­æ–‡æ¯è¯­è€…
4. **è¯­å¢ƒä¸°å¯Œ**: NVè‡ªç„¶åµŒå…¥åœ¨è¯­ä¹‰ä¸°å¯Œçš„å¥å­ä¸­

1. **Performative Recording**: Avoids ambiguity of NVs in spontaneous speech, ensures high-quality annotation
2. **Class Balance**: 17 NV categories with balanced distribution (max/min ratio only 2.7)
3. **Speaker Diversity**: 49 native Mandarin speakers from various regions  
4. **Rich Context**: NVs naturally embedded in semantically rich sentences

### è®¾è®¡åˆ›æ–° | Design Innovation

- **è„šæœ¬åŒ–æ–¹æ³•**: ä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­å¢ƒï¼Œç¡®ä¿NVçš„è¯­ä¹‰åˆç†æ€§
- **å¤šNVç»„åˆ**: æ”¯æŒ1-3ä¸ªNVçš„éšæœºç»„åˆï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
- **è¯´è¯äººç‹¬ç«‹åˆ’åˆ†**: ä¸¥æ ¼çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†ï¼Œç¡®ä¿æ³›åŒ–è¯„ä¼°

- **Scripted Approach**: LLM-generated natural contexts ensure semantic reasonableness of NVs
- **Multi-NV Combinations**: Supports random combinations of 1-3 NVs, simulating real scenarios
- **Speaker-Independent Split**: Strict train/validation/test division ensures generalization evaluation

è‰¯å¥½çš„æ•°æ®é›†è®¾è®¡å’Œå®è·µæ˜¯ä½¿å…¶èƒ½å¤Ÿåœ¨æ—¶é•¿æœ‰é™çš„æ ·æœ¬ä¸­è®­ç»ƒå‡ºæ³›åŒ–æ€§å¼ºçš„NV ASRæ¨¡å‹çš„å…³é”®åŸå› ä¹‹ä¸€ã€‚

The well-designed dataset and practices are one of the key reasons why it can train generalized NV ASR models with strong generalization from limited-duration samples.

## ç›¸å…³é“¾æ¥ | Related Links

- è®ºæ–‡: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- æ•°æ®é›†: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- å¾®è°ƒæ¨¡å‹: [å³å°†å‘å¸ƒ | Coming Soon]

## å¼•ç”¨ | Citation

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬æ•°æ®é›†æˆ–ç›¸å…³å·¥ä½œï¼Œè¯·å¼•ç”¨ï¼š

If you use this dataset or related work, please cite:

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech}, 
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

## ä¸»è¦è´¡çŒ® | Key Contributions

1. **é«˜è´¨é‡æ•°æ®é›†**: MNV-17æä¾›äº†ç›®å‰ç±»åˆ«æœ€å…¨é¢çš„ä¸­æ–‡å‰¯è¯­è¨€å‘å£°æ•°æ®é›†
2. **æ³›åŒ–èƒ½åŠ›éªŒè¯**: è¯æ˜äº†åŸºäºè¡¨æ¼”å¼æ•°æ®çš„æ¨¡å‹å…·æœ‰ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›
3. **ååŒæ•ˆåº”**: è¯æ˜NVè¯†åˆ«èƒ½åŠ›åœ¨å¤šä»»åŠ¡è®­ç»ƒä¸‹çš„è¯­éŸ³æ¨¡å‹ä¸­ä¸ä¼šæŸå®³ASRæ€§èƒ½
4. **åŸºå‡†å»ºç«‹**: ä¸ºNVæ„ŸçŸ¥ASRå»ºç«‹äº†æ ‡å‡†è¯„ä¼°åŸºå‡†

1. **High-Quality Dataset**: MNV-17 provides the most comprehensive Chinese nonverbal vocalization dataset
2. **Generalization Verification**: Proves excellent cross-speaker generalization of performative data-based models
3. **Synergy Discovery**: Proves NV recognition can enhance rather than harm ASR performance
4. **Benchmark Establishment**: Establishes standard evaluation benchmark for NV-aware ASR

---

**ä½œè€… | Authors**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**æœºæ„ | Institutions**: åå—ç†å·¥å¤§å­¦, é¦™æ¸¯ç†å·¥å¤§å­¦, åŒæµå¤§å­¦, ä¸Šæµ·äº¤é€šå¤§å­¦, é¦™æ¸¯ä¸­æ–‡å¤§å­¦, ä½›å±±å¤§å­¦