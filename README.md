# MNV-17 Qwen2.5-Omni Demo

[**ä¸­æ–‡ç‰ˆæœ¬**](#ä¸­æ–‡ç‰ˆæœ¬) | [**English Version**](#english-version)

---

## ä¸­æ–‡ç‰ˆæœ¬

### MNV-17 Qwen2.5-Omni æ¼”ç¤ºï¼šéè¯­è¨€å‘å£°è¯†åˆ«

æœ¬ä»“åº“å±•ç¤ºäº†åœ¨ [MNV-17 æ•°æ®é›†](https://huggingface.co/datasets/maimai11/MNV_17) ä¸Šå¾®è°ƒçš„ Qwen2.5-Omni æ¨¡å‹åœ¨éè¯­è¨€å‘å£°ï¼ˆNVï¼‰ASRè¯†åˆ«ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚

**[ç‚¹å‡»è¿™é‡Œä½“éªŒå¯æ’­æ”¾çš„éŸ³é¢‘æ¼”ç¤º](https://yongaifadian1.github.io/MNV-17/)**

### å…³é”®å‘ç°

#### æœªè§è¯´è¯äººæ³›åŒ–èƒ½åŠ›

**é‡è¦è¯´æ˜**: æ‰€æœ‰æ¼”ç¤ºæ ·æœ¬çš„è¯´è¯äººåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨æœªè§ã€‚

è¿™è¯æ˜äº†æ¨¡å‹å­¦ä¼šäº†**é€šç”¨çš„NVå‘å£°è§„å¾‹**ï¼Œè€Œéä»…ä»…æ‹Ÿåˆç‰¹å®šè¯´è¯äººçš„å‘å£°ä¹ æƒ¯ï¼Œå±•ç°äº†ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚

### æ¨¡å‹æ€§èƒ½

æ ¹æ®æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2509.18196)å®éªŒç»“æœï¼š

| æ¨¡å‹ Model | è”åˆCER | NVè¯†åˆ«å‡†ç¡®ç‡ |
|------------|---------|-------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

#### æ€§èƒ½äº®ç‚¹

1. **æœ€ä½è”åˆé”™è¯¯ç‡**: Qwen2.5-Omni å®ç°äº† 3.60% çš„è”åˆå­—ç¬¦é”™è¯¯ç‡ï¼Œåœ¨ ASR å’Œ NV è¯†åˆ«åŒé‡ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚
2. **å“è¶ŠNVè¯†åˆ«**: åœ¨ä¸¥æ ¼çš„å®Œå…¨åŒ¹é…è¯„ä¼°ä¸‹è¾¾åˆ° 57.29% å‡†ç¡®ç‡ï¼ˆç±»å‹ã€æ•°é‡ã€é¡ºåºå®Œå…¨åŒ¹é…ï¼‰ã€‚


### æ•°æ®é›†ç‰¹ç‚¹

#### MNV-17 æ•°æ®é›†ä¼˜åŠ¿

1. **è¡¨æ¼”å¼å½•åˆ¶**: é¿å…äº†è‡ªç„¶è¯­éŸ³ä¸­NVçš„æ¨¡ç³Šæ€§ï¼Œç¡®ä¿é«˜è´¨é‡æ ‡æ³¨ã€‚
2. **ç±»åˆ«å¹³è¡¡**: 17ä¸ªNVç±»åˆ«åˆ†å¸ƒå‡è¡¡ï¼ˆæœ€å¤§æœ€å°æ¯”ä¾‹ä»…2.7ï¼‰ã€‚
3. **è¯´è¯äººå¤šæ ·æ€§**: 49åæ¥è‡ªä¸åŒåœ°åŒºçš„ä¸­æ–‡æ¯è¯­è€…ã€‚
4. **è¯­å¢ƒä¸°å¯Œ**: NVè‡ªç„¶åµŒå…¥åœ¨è¯­ä¹‰ä¸°å¯Œçš„å¥å­ä¸­ã€‚

#### è®¾è®¡åˆ›æ–°

- **è„šæœ¬åŒ–æ–¹æ³•**: ä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­å¢ƒï¼Œç¡®ä¿NVçš„è¯­ä¹‰åˆç†æ€§ã€‚
- **å¤šNVç»„åˆ**: æ”¯æŒ1-3ä¸ªNVçš„éšæœºç»„åˆï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚
- **è¯´è¯äººç‹¬ç«‹åˆ’åˆ†**: ä¸¥æ ¼çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†ï¼Œç¡®ä¿æ³›åŒ–è¯„ä¼°ã€‚

### ç›¸å…³é“¾æ¥

- ğŸ“„ **è®ºæ–‡**: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- ğŸ¤— **æ•°æ®é›†**: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- ğŸ¤– **å¾®è°ƒæ¨¡å‹**: [å³å°†å‘å¸ƒ]

### å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬æ•°æ®é›†æˆ–ç›¸å…³å·¥ä½œï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech},
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

### ä¸»è¦è´¡çŒ®

1. **é«˜è´¨é‡æ•°æ®é›†**: MNV-17æä¾›äº†ç›®å‰ç±»åˆ«æœ€å…¨é¢çš„ä¸­æ–‡éè¯­è¨€å‘å£°æ•°æ®é›†ã€‚
2. **æ³›åŒ–èƒ½åŠ›éªŒè¯**: è¯æ˜äº†åŸºäºè¡¨æ¼”å¼æ•°æ®çš„æ¨¡å‹å…·æœ‰ä¼˜ç§€çš„è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ã€‚
3. **ååŒæ•ˆåº”**: è¯æ˜NVè¯†åˆ«èƒ½åŠ›åœ¨å¤šä»»åŠ¡è®­ç»ƒä¸‹çš„è¯­éŸ³æ¨¡å‹ä¸­ä¸ä¼šæŸå®³ASRæ€§èƒ½ã€‚
4. **åŸºå‡†å»ºç«‹**: ä¸ºNVæ„ŸçŸ¥ASRå»ºç«‹äº†æ ‡å‡†è¯„ä¼°åŸºå‡†ã€‚

---

**ä½œè€…**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**æœºæ„**: åå—ç†å·¥å¤§å­¦, é¦™æ¸¯ç†å·¥å¤§å­¦, åŒæµå¤§å­¦, ä¸Šæµ·äº¤é€šå¤§å­¦, é¦™æ¸¯ä¸­æ–‡å¤§å­¦, ä½›å±±å¤§å­¦

---

## English Version

### MNV-17 Qwen2.5-Omni Demo: Nonverbal Vocalization Recognition

This repository demonstrates the excellent performance of Qwen2.5-Omni model fine-tuned on the [MNV-17 dataset](https://huggingface.co/datasets/maimai11/MNV_17) for Nonverbal Vocalization (NV) ASR recognition tasks.

**[Click here for interactive audio demo](https://yongaifadian1.github.io/MNV-17/)**

### Key Findings

#### Unseen Speaker Generalization

**Crucial Note**: All demo samples are from speakers who were completely unseen during training.

This demonstrates that the model learned **universal NV vocalization patterns** rather than merely fitting specific speakers' habits, showcasing excellent cross-speaker generalization.

### Model Performance

According to our [paper](https://arxiv.org/abs/2509.18196) experimental results:

| Model | Joint CER | NV Recognition Accuracy |
|-------|-----------|------------------------|
| **Qwen2.5-Omni** | **3.60%** | **57.29%** |
| Qwen2-Audio | 4.84% | 56.28% |
| SenseVoice | 8.71% | 57.29% |
| Paraformer | 5.70% | 28.64% |

#### Performance Highlights

1. **Lowest Joint Error Rate**: Qwen2.5-Omni achieved 3.60% joint CER, best performance in dual ASR and NV recognition tasks.
2. **Excellent NV Recognition**: 57.29% accuracy under strict exact-match evaluation (type, count, order must all match).


### Dataset Characteristics

#### MNV-17 Dataset Advantages

1. **Performative Recording**: Avoids ambiguity of NVs in spontaneous speech, ensures high-quality annotation.
2. **Class Balance**: 17 NV categories with balanced distribution (max/min ratio only 2.7).
3. **Speaker Diversity**: 49 native Mandarin speakers from various regions.
4. **Rich Context**: NVs naturally embedded in semantically rich sentences.

#### Design Innovation

- **Scripted Approach**: LLM-generated natural contexts ensure semantic reasonableness of NVs.
- **Multi-NV Combinations**: Supports random combinations of 1-3 NVs, simulating real scenarios.
- **Speaker-Independent Split**: Strict train/validation/test division ensures generalization evaluation.

### Related Links

- ğŸ“„ **Paper**: [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
- ğŸ¤— **Dataset**: [maimai11/MNV_17](https://huggingface.co/datasets/maimai11/MNV_17)
- ğŸ¤– **Fine-tuned Model**: [Coming Soon]

### Citation

If you use this dataset or related work, please cite:

```bibtex
@misc{mai2025mnv17highqualityperformativemandarin,
      title={MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech},
      author={Jialong Mai and Jinxin Ji and Xiaofen Xing and Chen Yang and Weidong Chen and Jingyuan Xing and Xiangmin Xu},
      year={2025},
      eprint={2509.18196},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.18196}
}
```

### Key Contributions

1. **High-Quality Dataset**: MNV-17 provides the most comprehensive Chinese nonverbal vocalization dataset.
2. **Generalization Verification**: Proves excellent cross-speaker generalization of performative data-based models.
3. **Synergy Discovery**: Proves NV recognition can enhance rather than harm ASR performance.
4. **Benchmark Establishment**: Establishes standard evaluation benchmark for NV-aware ASR.

---

**Authors**: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu

**Institutions**: South China University of Technology, The Hong Kong Polytechnic University, Tongji University, Shanghai Jiao Tong University, The Chinese University of Hong Kong, Foshan University